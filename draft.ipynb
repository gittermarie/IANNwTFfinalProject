{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CheXNet applied to brain tumor MRI images used for binary classification of tumor vs no tumor.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.python.keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from loss import WeightedCrossEntropyBinaryLoss\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "import prepare_data\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# input_path = '../input/chest_xray/chest_xray/'\n",
    "input_path = 'C:/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/chest_xray/'\n",
    "\n",
    "# Checkpoint file path\n",
    "checkpoint_filepath = 'C:/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/Checkpoint' \n",
    "\n",
    "# Hyperparameters\n",
    "img_dims = 224\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "val_batch_size = 64\n",
    "\n",
    "# Getting the data\n",
    "train_gen, test_gen, test_data, test_labels = prepare_data.prepare_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXNet(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  The model using modified DenseNet121.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__ (self):\n",
    "    \"\"\"\n",
    "    The constructor initiates the layers and their activation functions....\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.batch_size = 16\n",
    "    self.input_size = 224\n",
    "    self.output_size = 1 # since it's a binary classification task\n",
    "    self.val_batch_size = 64  # This can be set any convenient value as per GPU capacity\n",
    "\n",
    "    # Following will be set by get_data_stats() based on the dataset\n",
    "    self.zero_weight = None\n",
    "    self.one_weight = None\n",
    "    self.train_steps = None\n",
    "    self.val_steps = None\n",
    "\n",
    "    # get_model() will initialize this to DenseNet121 model\n",
    "    self.model = None\n",
    "  \n",
    "  def get_data_stats(self, train_data_path, val_data_path, class_map):\n",
    "      \"\"\"\n",
    "      Computes normal Vs Pneumonia class distribution.\n",
    "      :param train_data_path: path to training data. Samples os each class should be stored in separate folders\n",
    "      :param val_data_path: path to validation data. Samples os each class should be stored in separate folders\n",
    "      :param class_map: mapping of class index to folder names\n",
    "      \"\"\"\n",
    "\n",
    "      # Count images in each class\n",
    "      for _set in ['train', 'val', 'test']:\n",
    "          n_normal = len(os.listdir(input_path + _set + '/NORMAL'))\n",
    "          n_infect = len(os.listdir(input_path + _set + '/PNEUMONIA'))\n",
    "\n",
    "      # compute class distribution\n",
    "      self.w_class1 = float(n_normal)/(n_normal+n_infect)\n",
    "      self.w_class0 = float(n_infect)/sum(n_normal+n_infect)\n",
    "\n",
    "      # For convenience at train time, compute number of steps required to complete an epoch\n",
    "      val_img_cnt = 0\n",
    "      for key, value in class_map.items():\n",
    "          imgs = glob.glob(val_data_path + value + \"/*.jpg\")\n",
    "          val_img_cnt += len(imgs)\n",
    "\n",
    "      self.train_steps = ((n_normal+n_infect) // self.batch_size) + 1\n",
    "      self.val_steps = ((n_normal+n_infect) // self.val_batch_size) + 1  \n",
    "\n",
    "\n",
    "  def get_model(self):\n",
    "    \n",
    "    # DenseNet121 expects number of channels to be 3\n",
    "    input = Input(shape=(self.input_size, self.input_size, 3))\n",
    "\n",
    "    # using pretrained DenseNet121 as the foundation of the model\n",
    "    self.base_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet', input_tensor=None,\n",
    "                                                                input_shape=None, pooling='avg', classes=2)\n",
    "    self.output_layer = tf.keras.layers.Dense(self.output_size, activation=tf.nn.sigmoid)\n",
    "\n",
    "\n",
    "    # Using weighted binary loss has been suggested in the paper\n",
    "    loss = WeightedCrossEntropyBinaryLoss(self.zero_weight, self.one_weight)\n",
    "\n",
    "    # Compile the model\n",
    "    self.model=model.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999),\n",
    "                  loss=loss.weighted_binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "  @tf.function\n",
    "  def call (self, input):\n",
    "    \"\"\"\n",
    "    This function calls the model on new input and returns the output as tensors.\n",
    "\n",
    "    Arguments:\n",
    "    input -- denotes the input tensors\n",
    "    \"\"\"\n",
    "    x = self.base_model(input)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "# Instantiate the CheXNet model\n",
    "model = CheXNet()\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\Osnabruck\\IANNwTFfinalProject\\draft.ipynb Cell 4\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Fitting the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_gen, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mtest_gen,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[reduce_lr, model_checkpoint])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load the model with the lowest validation loss\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/IANNwTFfinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mload_weights(checkpoint_filepath)\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\marta\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\engine\\training.py:3618\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   3613\u001b[0m     \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[0;32m   3615\u001b[0m     \u001b[39m# model is compiled\u001b[39;00m\n\u001b[0;32m   3616\u001b[0m     \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[0;32m   3617\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[1;32m-> 3618\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3619\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3620\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3621\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3622\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "#Fitting the model\n",
    "history = model.fit(\n",
    "    train_gen, epochs=10, validation_data=test_gen,\n",
    "    callbacks=[reduce_lr, model_checkpoint])\n",
    "\n",
    "# Load the model with the lowest validation loss\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
