{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marta\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CheXNet applied to brain tumor MRI images used for binary classification of tumor vs no tumor.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "#import tensorflow.python.keras.applications\n",
    "import tensorflow.python.keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from loss import WeightedCrossEntropyBinaryLoss\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = '../input/chest_xray/chest_xray/'\n",
    "input_path = 'C:/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/chest_xray/'\n",
    "\n",
    "# Checkpoint file path\n",
    "checkpoint_filepath = '/path/to/checkpoint' \n",
    "\n",
    "# Hyperparameters\n",
    "img_dims = 224\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "val_batch_size = 64\n",
    "\n",
    "# Getting the data\n",
    "train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\Osnabruck\\ImplementingANNswithTensorFlow\\FinalProject\\draft.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Instantiate the CheXNet model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m model \u001b[39m=\u001b[39m CheXNet()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m### the text below was written by Chat GPT\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(beta_1\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, beta_2\u001b[39m=\u001b[39m\u001b[39m0.999\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\marta\\OneDrive\\Desktop\\Osnabruck\\ImplementingANNswithTensorFlow\\FinalProject\\draft.ipynb Cell 3\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m WeightedCrossEntropyBinaryLoss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mone_weight)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Note: default learning rate of 'adam' is 0.001 as required by the paper\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mweighted_binary_crossentropy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/draft.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "class CheXNet(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  The model using modified DenseNet121.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__ (self):\n",
    "    \"\"\"\n",
    "    The constructor initiates the layers and their activation functions....\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.batch_size = 16\n",
    "    self.input_size = 224\n",
    "    self.output_size = 1 # since it's a binary classification task\n",
    "    self.val_batch_size = 64  # This can be set any convenient value as per GPU capacity\n",
    "\n",
    "    # Following will be set by get_data_stats() based on the dataset\n",
    "    self.zero_weight = None\n",
    "    self.one_weight = None\n",
    "    self.train_steps = None\n",
    "    self.val_steps = None\n",
    "\n",
    "    # get_model() will initialize this to DenseNet121 model\n",
    "    self.model = None\n",
    "  \n",
    "  def get_data_stats(self, train_data_path, val_data_path, class_map):\n",
    "      \"\"\"\n",
    "      Computes normal Vs Pneumonia class distribution\n",
    "      :param train_data_path: path to training data. Samples os each class should be stored in separate folders\n",
    "      :param val_data_path: path to validation data. Samples os each class should be stored in separate folders\n",
    "      :param class_map: mapping of class index to folder names\n",
    "      \"\"\"\n",
    "\n",
    "      # Count images in each class\n",
    "      for _set in ['train', 'val', 'test']:\n",
    "          n_normal = len(os.listdir(input_path + _set + '/NORMAL'))\n",
    "          n_infect = len(os.listdir(input_path + _set + '/PNEUMONIA'))\n",
    "\n",
    "      # compute class distribution\n",
    "      self.w_class1 = float(n_normal)/(n_normal+n_infect)\n",
    "      self.w_class0 = float(n_infect)/sum(n_normal+n_infect)\n",
    "\n",
    "      # For convenience at train time, compute number of steps required to complete an epoch\n",
    "      val_img_cnt = 0\n",
    "      for key, value in class_map.items():\n",
    "          imgs = glob.glob(val_data_path + value + \"/*.jpg\")\n",
    "          val_img_cnt += len(imgs)\n",
    "\n",
    "      self.train_steps = ((n_normal+n_infect) // self.batch_size) + 1\n",
    "      self.val_steps = ((n_normal+n_infect) // self.val_batch_size) + 1  \n",
    "\n",
    "\n",
    "  def get_model(self):\n",
    "    \n",
    "    # DenseNet121 expects number of channels to be 3\n",
    "    input = Input(shape=(self.input_size, self.input_size, 3))\n",
    "\n",
    "    # using pretrained DenseNet121 as the foundation of the model\n",
    "    self.base_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet', input_tensor=None,\n",
    "                                                                input_shape=None, pooling='avg', classes=2)\n",
    "    self.output_layer = tf.keras.layers.Dense(self.output_size, activation=tf.nn.sigmoid)\n",
    "\n",
    "\n",
    "    # Using weighted binary loss has been suggested in the paper\n",
    "    loss = WeightedCrossEntropyBinaryLoss(self.zero_weight, self.one_weight)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999),\n",
    "                  loss=loss.weighted_binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "  @tf.function\n",
    "  def call (self, input):\n",
    "    \"\"\"\n",
    "    This function calls the model on new input and returns the output as tensors.\n",
    "\n",
    "    Arguments:\n",
    "    input -- denotes the input tensors\n",
    "    \"\"\"\n",
    "    x = self.base_model(input)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "# Instantiate the CheXNet model\n",
    "model = CheXNet()\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "history = model.fit(\n",
    "    train_generator, epochs=10, validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, model_checkpoint])\n",
    "\n",
    "# Load the model with the lowest validation loss\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
