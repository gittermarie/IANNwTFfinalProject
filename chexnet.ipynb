{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CheXNet applied to chest X-ray images classifying pneumonia vs normal.\n",
    "\n",
    "Reimplementation of the paper: \n",
    "https://doi.org/10.48550/arXiv.1711.05225\n",
    "\n",
    "The dataset can be downloaded from here:\n",
    "https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from loss import WeightedCrossEntropyBinaryLoss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import prepare_data\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path to the input that has to be specified\n",
    "input_path = 'C:/Users/marta/OneDrive/Desktop/Osnabruck/ImplementingANNswithTensorFlow/FinalProject/chest_xray/'\n",
    "\n",
    "# Checkpoint file path\n",
    "checkpoint_filepath = './checkpoint/weights.hdf5' \n",
    "\n",
    "# Train data path\n",
    "train_data_path = input_path+'train'\n",
    "\n",
    "# Validation data path\n",
    "val_data_path = input_path+'val'\n",
    "\n",
    "# Testing data path\n",
    "test_data_path=input_path+'test'\n",
    "\n",
    "# Tensorboard log files path\n",
    "log_dir='./logs'\n",
    "\n",
    "# Hyperparameters set in accordance with the methods of the original paper\n",
    "img_dims = 224\n",
    "n_epochs = 10\n",
    "batch_size = 16 \n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been split. Skipping data split.\n"
     ]
    }
   ],
   "source": [
    "# Adding files from the training dataset since the current validation set is only 0.03% of the training set\n",
    "prepare_data.split_data(0.1, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4695 images belonging to 2 classes.\n",
      "Found 537 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "train_gen, val_gen, test_data, test_labels = prepare_data.prepare_data(img_dims, batch_size, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXNet(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  The CheXNet model that uses DenseNet121 as its backbone.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__ (self):\n",
    "    \"\"\"\n",
    "    The constructor instantiates the weights and the model.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    # Instantiate weights and steps and set them to None\n",
    "    self.zero_weight = None\n",
    "    self.one_weight = None\n",
    "    self.train_steps = None\n",
    "    self.val_steps = None\n",
    "\n",
    "    # get_model() will initialize this to DenseNet121 model\n",
    "    self.model = None\n",
    "  \n",
    "  def get_weights(self, train_data_path):\n",
    "    \"\"\"\n",
    "    Computes class distribution of pneumonia vs normal images.\n",
    "\n",
    "    Args:\n",
    "      train_data_path: path to training data.\n",
    "      val_data_path: path to validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count images in each class in the train data\n",
    "    n_normal = len(os.listdir(train_data_path + '/NORMAL'))\n",
    "    n_pneumonia = len(os.listdir(train_data_path + '/PNEUMONIA'))\n",
    "\n",
    "    # Compute class distribution\n",
    "    self.one_weight = float(n_normal)/(n_normal+n_pneumonia)\n",
    "    self.zero_weight = float(n_pneumonia)/(n_normal+n_pneumonia)\n",
    "\n",
    "\n",
    "  def get_model(self):\n",
    "\n",
    "    # DenseNet121 expects number of channels to be 3\n",
    "    input = Input(shape=(img_dims, img_dims, 3), batch_size=batch_size)\n",
    "\n",
    "    # using pretrained DenseNet121 as the foundation of the model\n",
    "    base_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet',\n",
    "                                                            input_shape=(img_dims, img_dims, 3), pooling='avg')\n",
    "    \n",
    "    # Add custom output layers\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.Dense(output_size, activation='sigmoid')(x)\n",
    "\n",
    "    self.model = tf.keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    # Use weighted binary crossentropy loss\n",
    "    loss = WeightedCrossEntropyBinaryLoss(self.zero_weight, self.one_weight)\n",
    "\n",
    "    # Compile the model\n",
    "    self.model.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999),\n",
    "                       loss=loss.weighted_binary_crossentropy,\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    return self.model\n",
    "\n",
    "# Instantiate the CheXNet model\n",
    "chexnet = CheXNet()\n",
    "\n",
    "# Compute class distribution of pneumonia vs normal images\n",
    "chexnet.get_weights(train_data_path)\n",
    "\n",
    "# Create and compile the DenseNet121 model\n",
    "model = chexnet.get_model()\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "The model has already been trained via the cell below.\n",
    "\n",
    "For time-efficiency it can be loaded for evaluation on the test set from a checkpoint file in the next cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "293/293 [==============================] - 781s 3s/step - loss: 0.0993 - accuracy: 0.8996 - val_loss: 0.4097 - val_accuracy: 0.3636 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9297\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "293/293 [==============================] - 789s 3s/step - loss: 0.0697 - accuracy: 0.9297 - val_loss: 0.7279 - val_accuracy: 0.4602 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 789s 3s/step - loss: 0.0522 - accuracy: 0.9470 - val_loss: 0.0577 - val_accuracy: 0.9545 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9590\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "293/293 [==============================] - 767s 3s/step - loss: 0.0398 - accuracy: 0.9590 - val_loss: 0.2061 - val_accuracy: 0.7254 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9643\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "293/293 [==============================] - 765s 3s/step - loss: 0.0351 - accuracy: 0.9643 - val_loss: 0.0673 - val_accuracy: 0.9223 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9626\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "293/293 [==============================] - 765s 3s/step - loss: 0.0365 - accuracy: 0.9626 - val_loss: 0.0930 - val_accuracy: 0.8902 - lr: 1.0000e-06\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9645\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "293/293 [==============================] - 764s 3s/step - loss: 0.0352 - accuracy: 0.9645 - val_loss: 0.0936 - val_accuracy: 0.8883 - lr: 1.0000e-07\n",
      "Epoch 8/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9641\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "293/293 [==============================] - 779s 3s/step - loss: 0.0360 - accuracy: 0.9641 - val_loss: 0.0921 - val_accuracy: 0.8902 - lr: 1.0000e-08\n",
      "Epoch 9/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9718\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "293/293 [==============================] - 781s 3s/step - loss: 0.0296 - accuracy: 0.9718 - val_loss: 0.0906 - val_accuracy: 0.8958 - lr: 1.0000e-09\n",
      "Epoch 10/10\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9722\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "293/293 [==============================] - 783s 3s/step - loss: 0.0287 - accuracy: 0.9722 - val_loss: 0.0910 - val_accuracy: 0.8939 - lr: 1.0000e-10\n"
     ]
    }
   ],
   "source": [
    "# # Fitting the model\n",
    "# history = model.fit(train_gen,\n",
    "#                     epochs=n_epochs,\n",
    "#                     batch_size=batch_size,\n",
    "#                     steps_per_epoch=train_gen.samples // batch_size,\n",
    "#                     validation_steps=val_gen.samples // batch_size,\n",
    "#                     validation_data=val_gen,\n",
    "#                     callbacks=[reduce_lr, model_checkpoint, tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 20s 519ms/step\n",
      "\n",
      "Model evaluation\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179  55]\n",
      " [  4 386]]\n",
      "\n",
      "Accuracy: 90.5448717948718%\n",
      "Precision: 87.52834467120182%\n",
      "Recall: 98.97435897435898%\n",
      "F1 Score: 92.90012033694344\n"
     ]
    }
   ],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# Compute predictions and round to obtain binary predictions\n",
    "yhat = np.round(model.predict(test_data, batch_size=batch_size))\n",
    "\n",
    "# Compute accuracy, confusion matrix, and metrics from confusion matrix\n",
    "acc = accuracy_score(test_labels, yhat) * 100\n",
    "tn, fp, fn, tp = confusion_matrix(test_labels, yhat).ravel()\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(test_labels, yhat, average='binary')\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nModel evaluation\\n\") \n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(test_labels, yhat)}\\n\")\n",
    "print(f\"Accuracy: {acc}%\")\n",
    "print(f\"Precision: {precision * 100}%\")\n",
    "print(f\"Recall: {recall * 100}%\")\n",
    "print(f\"F1 Score: {f1_score * 100}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
